{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformers & Attention Mechanisms\n",
                "\n",
                "## Week 3, Day 3 - Understanding Modern NLP Architecture\n",
                "\n",
                "### What You'll Learn:\n",
                "1. **Attention Mechanism** - The breakthrough idea\n",
                "2. **Self-Attention** - Query, Key, Value\n",
                "3. **Multi-Head Attention** - Multiple perspectives\n",
                "4. **Transformer Architecture** - Encoder-Decoder\n",
                "5. **üéØ Practical**: Text Classification with BERT\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Transformers?\n",
                "\n",
                "### Problems with RNNs/LSTMs:\n",
                "\n",
                "```\n",
                "Sequential Processing:\n",
                "Word 1 ‚Üí Word 2 ‚Üí Word 3 ‚Üí Word 4 ‚Üí Word 5\n",
                "  ‚Üì        ‚Üì        ‚Üì        ‚Üì        ‚Üì\n",
                "Can't parallelize! Must process one at a time.\n",
                "```\n",
                "\n",
                "**Key Issues:**\n",
                "- ‚ùå Sequential processing (slow)\n",
                "- ‚ùå Long-range dependencies fade\n",
                "- ‚ùå Vanishing gradients\n",
                "- ‚ùå Can't use GPU parallelization effectively\n",
                "\n",
                "### The Transformer Solution:\n",
                "\n",
                "```\n",
                "Parallel Processing:\n",
                "Word 1 ‚îÄ‚îÄ‚îê\n",
                "Word 2 ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí Process ALL words simultaneously!\n",
                "Word 3 ‚îÄ‚îÄ‚î§\n",
                "Word 4 ‚îÄ‚îÄ‚î§\n",
                "Word 5 ‚îÄ‚îÄ‚îò\n",
                "```\n",
                "\n",
                "**Advantages:**\n",
                "- ‚úÖ Parallel processing (fast)\n",
                "- ‚úÖ Direct connections between all words\n",
                "- ‚úÖ Better long-range dependencies\n",
                "- ‚úÖ Scales with GPU power"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Attention Mechanism Intuition\n",
                "\n",
                "### The Core Idea:\n",
                "\n",
                "When reading: **\"The animal didn't cross the street because it was too tired\"**\n",
                "\n",
                "What does **\"it\"** refer to?\n",
                "\n",
                "```\n",
                "Attention weights:\n",
                "The      ‚Üí 0.01\n",
                "animal   ‚Üí 0.85  ‚Üê High attention!\n",
                "didn't   ‚Üí 0.02\n",
                "cross    ‚Üí 0.03\n",
                "street   ‚Üí 0.05\n",
                "because  ‚Üí 0.01\n",
                "it       ‚Üí 0.00\n",
                "was      ‚Üí 0.01\n",
                "too      ‚Üí 0.01\n",
                "tired    ‚Üí 0.01\n",
                "```\n",
                "\n",
                "**Attention helps the model focus on relevant words!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "# !pip install transformers datasets torch matplotlib numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"GPU available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Scaled Dot-Product Attention\n",
                "\n",
                "### The Formula:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "### Components:\n",
                "\n",
                "| Component | Meaning |\n",
                "|-----------|----------|\n",
                "| **Q** (Query) | \"What am I looking for?\" |\n",
                "| **K** (Key) | \"What do I contain?\" |\n",
                "| **V** (Value) | \"What information do I have?\" |\n",
                "| **$d_k$** | Dimension of keys (for scaling) |\n",
                "\n",
                "### Analogy: Library Search\n",
                "\n",
                "```\n",
                "You (Query):     \"I need books about machine learning\"\n",
                "Book Labels (Keys):  [\"ML\", \"Cooking\", \"History\", \"AI\"]\n",
                "Book Content (Values): [ML_book, Cook_book, Hist_book, AI_book]\n",
                "\n",
                "Attention scores: [0.6, 0.0, 0.0, 0.4]\n",
                "Result: 0.6 * ML_book + 0.4 * AI_book\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
                "    \"\"\"\n",
                "    Compute scaled dot-product attention.\n",
                "    \n",
                "    Args:\n",
                "        Q: Query matrix (batch_size, seq_len, d_k)\n",
                "        K: Key matrix (batch_size, seq_len, d_k)\n",
                "        V: Value matrix (batch_size, seq_len, d_v)\n",
                "        mask: Optional mask\n",
                "    \n",
                "    Returns:\n",
                "        output: Attention output\n",
                "        attention_weights: Attention weights for visualization\n",
                "    \"\"\"\n",
                "    d_k = Q.size(-1)\n",
                "    \n",
                "    # Step 1: Compute attention scores\n",
                "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
                "    \n",
                "    # Step 2: Apply mask (if provided)\n",
                "    if mask is not None:\n",
                "        scores = scores.masked_fill(mask == 0, -1e9)\n",
                "    \n",
                "    # Step 3: Apply softmax\n",
                "    attention_weights = F.softmax(scores, dim=-1)\n",
                "    \n",
                "    # Step 4: Multiply by values\n",
                "    output = torch.matmul(attention_weights, V)\n",
                "    \n",
                "    return output, attention_weights\n",
                "\n",
                "# Test with simple example\n",
                "seq_len, d_k = 5, 8\n",
                "Q = torch.randn(1, seq_len, d_k)\n",
                "K = torch.randn(1, seq_len, d_k)\n",
                "V = torch.randn(1, seq_len, d_k)\n",
                "\n",
                "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(f\"Input shape: {Q.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
                "print(f\"\\nAttention weights (should sum to 1):\\n{attn_weights[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention weights\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.imshow(attn_weights[0].detach().numpy(), cmap='viridis')\n",
                "plt.colorbar(label='Attention Weight')\n",
                "plt.xlabel('Key Position')\n",
                "plt.ylabel('Query Position')\n",
                "plt.title('Attention Weight Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Self-Attention\n",
                "\n",
                "### What is Self-Attention?\n",
                "\n",
                "**Self-Attention**: Each word attends to all other words in the same sentence.\n",
                "\n",
                "```\n",
                "Sentence: \"The cat sat on the mat\"\n",
                "\n",
                "For word \"sat\":\n",
                "  - How much attention to \"The\"? ‚Üí 0.05\n",
                "  - How much attention to \"cat\"? ‚Üí 0.60 (subject!)\n",
                "  - How much attention to \"sat\"? ‚Üí 0.10 (itself)\n",
                "  - How much attention to \"on\"?  ‚Üí 0.15\n",
                "  - How much attention to \"the\"? ‚Üí 0.05\n",
                "  - How much attention to \"mat\"? ‚Üí 0.05\n",
                "```\n",
                "\n",
                "### Key Insight:\n",
                "Q, K, V all come from the **same input** (hence \"self\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    def __init__(self, embed_dim):\n",
                "        super().__init__()\n",
                "        self.embed_dim = embed_dim\n",
                "        \n",
                "        # Linear transformations for Q, K, V\n",
                "        self.query = nn.Linear(embed_dim, embed_dim)\n",
                "        self.key = nn.Linear(embed_dim, embed_dim)\n",
                "        self.value = nn.Linear(embed_dim, embed_dim)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: Input tensor (batch_size, seq_len, embed_dim)\n",
                "        Returns:\n",
                "            output: Attention output\n",
                "            attention_weights: For visualization\n",
                "        \"\"\"\n",
                "        # Generate Q, K, V from same input\n",
                "        Q = self.query(x)\n",
                "        K = self.key(x)\n",
                "        V = self.value(x)\n",
                "        \n",
                "        # Apply scaled dot-product attention\n",
                "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
                "        \n",
                "        return output, attention_weights\n",
                "\n",
                "# Test self-attention\n",
                "embed_dim = 64\n",
                "seq_len = 10\n",
                "batch_size = 2\n",
                "\n",
                "self_attn = SelfAttention(embed_dim)\n",
                "x = torch.randn(batch_size, seq_len, embed_dim)\n",
                "\n",
                "output, attn_weights = self_attn(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attn_weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Multi-Head Attention\n",
                "\n",
                "### Why Multiple Heads?\n",
                "\n",
                "Different heads can focus on different aspects:\n",
                "\n",
                "```\n",
                "Sentence: \"The quick brown fox jumps\"\n",
                "\n",
                "Head 1: Focus on syntax\n",
                "  \"quick\" ‚Üí \"fox\" (adjective-noun)\n",
                "  \n",
                "Head 2: Focus on semantics  \n",
                "  \"fox\" ‚Üí \"jumps\" (subject-verb)\n",
                "  \n",
                "Head 3: Focus on position\n",
                "  Each word ‚Üí nearby words\n",
                "```\n",
                "\n",
                "### Architecture:\n",
                "\n",
                "```\n",
                "Input\n",
                "  ‚Üì\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇHead1‚îÇHead2‚îÇHead3‚îÇHead4‚îÇ  ‚Üê Multiple attention heads\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "  ‚Üì\n",
                "Concatenate\n",
                "  ‚Üì\n",
                "Linear Layer\n",
                "  ‚Üì\n",
                "Output\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, embed_dim, num_heads):\n",
                "        super().__init__()\n",
                "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
                "        \n",
                "        self.embed_dim = embed_dim\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = embed_dim // num_heads\n",
                "        \n",
                "        # Linear layers for Q, K, V\n",
                "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
                "        self.out = nn.Linear(embed_dim, embed_dim)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        batch_size, seq_len, embed_dim = x.shape\n",
                "        \n",
                "        # Generate Q, K, V\n",
                "        qkv = self.qkv(x)  # (batch, seq_len, 3*embed_dim)\n",
                "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
                "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq_len, head_dim)\n",
                "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
                "        \n",
                "        # Scaled dot-product attention for each head\n",
                "        d_k = self.head_dim\n",
                "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
                "        attn_weights = F.softmax(scores, dim=-1)\n",
                "        attn_output = torch.matmul(attn_weights, V)\n",
                "        \n",
                "        # Concatenate heads\n",
                "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
                "        attn_output = attn_output.reshape(batch_size, seq_len, embed_dim)\n",
                "        \n",
                "        # Final linear layer\n",
                "        output = self.out(attn_output)\n",
                "        \n",
                "        return output, attn_weights\n",
                "\n",
                "# Test multi-head attention\n",
                "embed_dim = 64\n",
                "num_heads = 8\n",
                "seq_len = 10\n",
                "batch_size = 2\n",
                "\n",
                "mha = MultiHeadAttention(embed_dim, num_heads)\n",
                "x = torch.randn(batch_size, seq_len, embed_dim)\n",
                "\n",
                "output, attn_weights = mha(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
                "print(f\"Number of heads: {num_heads}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Positional Encoding\n",
                "\n",
                "### The Problem:\n",
                "\n",
                "Attention has **no notion of order**!\n",
                "\n",
                "```\n",
                "\"Dog bites man\" = \"Man bites dog\" (to attention!)\n",
                "```\n",
                "\n",
                "### Solution: Positional Encoding\n",
                "\n",
                "Add position information to embeddings:\n",
                "\n",
                "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
                "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
                "\n",
                "Where:\n",
                "- `pos` = position in sequence\n",
                "- `i` = dimension index\n",
                "- `d` = embedding dimension"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def positional_encoding(seq_len, d_model):\n",
                "    \"\"\"\n",
                "    Generate positional encoding.\n",
                "    \n",
                "    Args:\n",
                "        seq_len: Sequence length\n",
                "        d_model: Embedding dimension\n",
                "    \n",
                "    Returns:\n",
                "        Positional encoding matrix (seq_len, d_model)\n",
                "    \"\"\"\n",
                "    position = np.arange(seq_len)[:, np.newaxis]\n",
                "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
                "    \n",
                "    pe = np.zeros((seq_len, d_model))\n",
                "    pe[:, 0::2] = np.sin(position * div_term)\n",
                "    pe[:, 1::2] = np.cos(position * div_term)\n",
                "    \n",
                "    return torch.FloatTensor(pe)\n",
                "\n",
                "# Visualize positional encoding\n",
                "seq_len = 100\n",
                "d_model = 128\n",
                "pe = positional_encoding(seq_len, d_model)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.imshow(pe.numpy(), cmap='RdBu', aspect='auto')\n",
                "plt.colorbar(label='Value')\n",
                "plt.xlabel('Embedding Dimension')\n",
                "plt.ylabel('Position in Sequence')\n",
                "plt.title('Positional Encoding Visualization')\n",
                "plt.show()\n",
                "\n",
                "print(f\"Positional encoding shape: {pe.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Transformer Block\n",
                "\n",
                "### Complete Transformer Encoder Block:\n",
                "\n",
                "```\n",
                "Input\n",
                "  ‚Üì\n",
                "Multi-Head Attention\n",
                "  ‚Üì\n",
                "Add & Normalize (Residual)\n",
                "  ‚Üì\n",
                "Feed-Forward Network\n",
                "  ‚Üì\n",
                "Add & Normalize (Residual)\n",
                "  ‚Üì\n",
                "Output\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerBlock(nn.Module):\n",
                "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Multi-head attention\n",
                "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
                "        \n",
                "        # Feed-forward network\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(embed_dim, ff_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(ff_dim, embed_dim)\n",
                "        )\n",
                "        \n",
                "        # Layer normalization\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        \n",
                "        # Dropout\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Multi-head attention with residual\n",
                "        attn_output, attn_weights = self.attention(x)\n",
                "        x = self.norm1(x + self.dropout(attn_output))\n",
                "        \n",
                "        # Feed-forward with residual\n",
                "        ffn_output = self.ffn(x)\n",
                "        x = self.norm2(x + self.dropout(ffn_output))\n",
                "        \n",
                "        return x, attn_weights\n",
                "\n",
                "# Test transformer block\n",
                "embed_dim = 64\n",
                "num_heads = 8\n",
                "ff_dim = 256\n",
                "seq_len = 10\n",
                "batch_size = 2\n",
                "\n",
                "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
                "x = torch.randn(batch_size, seq_len, embed_dim)\n",
                "\n",
                "output, attn_weights = transformer_block(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(\"‚úÖ Transformer block working!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Introduction to HuggingFace ü§ó\n",
                "\n",
                "### What is HuggingFace?\n",
                "\n",
                "**HuggingFace** is the GitHub of AI models:\n",
                "- 100,000+ pre-trained models\n",
                "- Easy-to-use APIs\n",
                "- State-of-the-art NLP, Vision, Audio models\n",
                "\n",
                "### Popular Models:\n",
                "- **BERT**: Text understanding\n",
                "- **GPT-2/GPT-3**: Text generation\n",
                "- **T5**: Text-to-text tasks\n",
                "- **RoBERTa**: Improved BERT\n",
                "- **DistilBERT**: Faster, smaller BERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModel\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Load pre-trained BERT model\n",
                "model_name = \"bert-base-uncased\"\n",
                "\n",
                "print(f\"Loading {model_name}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModel.from_pretrained(model_name)\n",
                "\n",
                "print(f\"‚úÖ Model loaded!\")\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Tokenization with Transformers\n",
                "\n",
                "### How BERT Tokenizes:\n",
                "\n",
                "```\n",
                "Input: \"Transformers are amazing!\"\n",
                "\n",
                "Tokens: [CLS] transform ##ers are amazing ! [SEP]\n",
                "IDs:    [101]  [19081] [2545] [2024] [6429] [999] [102]\n",
                "```\n",
                "\n",
                "**Special Tokens:**\n",
                "- `[CLS]`: Start of sequence\n",
                "- `[SEP]`: Separator/End of sequence\n",
                "- `[PAD]`: Padding token\n",
                "- `[MASK]`: Masked token (for training)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example tokenization\n",
                "text = \"Transformers revolutionized natural language processing!\"\n",
                "\n",
                "# Tokenize\n",
                "tokens = tokenizer.tokenize(text)\n",
                "print(f\"Original text: {text}\")\n",
                "print(f\"\\nTokens: {tokens}\")\n",
                "\n",
                "# Convert to IDs\n",
                "token_ids = tokenizer.encode(text)\n",
                "print(f\"\\nToken IDs: {token_ids}\")\n",
                "\n",
                "# Decode back\n",
                "decoded = tokenizer.decode(token_ids)\n",
                "print(f\"\\nDecoded: {decoded}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Batch tokenization with padding\n",
                "sentences = [\n",
                "    \"I love transformers!\",\n",
                "    \"BERT is powerful.\",\n",
                "    \"Natural language processing has been revolutionized by attention mechanisms.\"\n",
                "]\n",
                "\n",
                "# Tokenize with padding and truncation\n",
                "encoded = tokenizer(\n",
                "    sentences,\n",
                "    padding=True,\n",
                "    truncation=True,\n",
                "    max_length=20,\n",
                "    return_tensors='pt'\n",
                ")\n",
                "\n",
                "print(\"Input IDs shape:\", encoded['input_ids'].shape)\n",
                "print(\"Attention mask shape:\", encoded['attention_mask'].shape)\n",
                "print(\"\\nInput IDs:\\n\", encoded['input_ids'])\n",
                "print(\"\\nAttention Mask:\\n\", encoded['attention_mask'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. üéØ Project: Sentiment Analysis with BERT\n",
                "\n",
                "Let's build a sentiment classifier using pre-trained BERT!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load IMDB dataset\n",
                "print(\"Loading IMDB dataset...\")\n",
                "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")  # Small subset for demo\n",
                "test_dataset = load_dataset(\"imdb\", split=\"test[:200]\")\n",
                "\n",
                "print(f\"Training samples: {len(dataset)}\")\n",
                "print(f\"Test samples: {len(test_dataset)}\")\n",
                "print(f\"\\nExample: {dataset[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tokenize dataset\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256)\n",
                "\n",
                "print(\"Tokenizing dataset...\")\n",
                "tokenized_train = dataset.map(tokenize_function, batched=True)\n",
                "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
                "\n",
                "print(\"‚úÖ Tokenization complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load BERT for classification\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    \"bert-base-uncased\",\n",
                "    num_labels=2  # Binary classification (positive/negative)\n",
                ")\n",
                "\n",
                "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=8,\n",
                "    per_device_eval_batch_size=8,\n",
                "    num_train_epochs=2,\n",
                "    weight_decay=0.01,\n",
                "    logging_dir='./logs',\n",
                ")\n",
                "\n",
                "# Metric function\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    logits, labels = eval_pred\n",
                "    predictions = np.argmax(logits, axis=-1)\n",
                "    return {'accuracy': accuracy_score(labels, predictions)}\n",
                "\n",
                "# Create trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_train,\n",
                "    eval_dataset=tokenized_test,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "print(\"üöÄ Starting training...\")\n",
                "trainer.train()\n",
                "print(\"‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on custom sentences\n",
                "def predict_sentiment(text):\n",
                "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
                "    outputs = model(**inputs)\n",
                "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
                "    sentiment = \"Positive üòä\" if prediction == 1 else \"Negative üòû\"\n",
                "    confidence = torch.softmax(outputs.logits, dim=-1)[0][prediction].item()\n",
                "    return sentiment, confidence\n",
                "\n",
                "# Test examples\n",
                "test_sentences = [\n",
                "    \"This movie was absolutely fantastic! I loved every minute.\",\n",
                "    \"Terrible film. Waste of time and money.\",\n",
                "    \"It was okay, nothing special.\",\n",
                "    \"Best movie I've seen this year!\"\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"SENTIMENT ANALYSIS RESULTS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for sentence in test_sentences:\n",
                "    sentiment, confidence = predict_sentiment(sentence)\n",
                "    print(f\"\\nText: {sentence}\")\n",
                "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.2%})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Transformer vs RNN Comparison\n",
                "\n",
                "| Feature | RNN/LSTM | Transformer |\n",
                "|---------|----------|-------------|\n",
                "| **Processing** | Sequential | Parallel |\n",
                "| **Speed** | Slow | Fast |\n",
                "| **Long-range dependencies** | Difficult | Easy |\n",
                "| **GPU utilization** | Poor | Excellent |\n",
                "| **Training time** | Long | Shorter |\n",
                "| **Memory** | Lower | Higher |\n",
                "| **Interpretability** | Hard | Easier (attention weights) |\n",
                "\n",
                "### When to use what?\n",
                "\n",
                "**Use RNN/LSTM:**\n",
                "- Small datasets\n",
                "- Limited compute\n",
                "- Online/streaming data\n",
                "\n",
                "**Use Transformers:**\n",
                "- Large datasets\n",
                "- GPU available\n",
                "- State-of-the-art performance needed"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Summary & Key Takeaways\n",
                "\n",
                "### What We Learned:\n",
                "\n",
                "‚úÖ **Attention Mechanism**: Focus on relevant parts of input\n",
                "\n",
                "‚úÖ **Self-Attention**: Each word attends to all others\n",
                "\n",
                "‚úÖ **Multi-Head Attention**: Multiple perspectives simultaneously\n",
                "\n",
                "‚úÖ **Positional Encoding**: Adding position information\n",
                "\n",
                "‚úÖ **Transformer Architecture**: Complete encoder block\n",
                "\n",
                "‚úÖ **HuggingFace**: Using pre-trained models\n",
                "\n",
                "‚úÖ **Practical Application**: Sentiment analysis with BERT\n",
                "\n",
                "### Key Formula:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "### Next Steps:\n",
                "- **Day 4**: GPT architecture and text generation\n",
                "- **Day 5**: Fine-tuning transformers for custom tasks\n",
                "- **Week 4**: Large Language Models (BERT, GPT, T5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. üìù Practice Exercises\n",
                "\n",
                "### Exercise 1: Visualize Attention\n",
                "Create a heatmap showing which words attend to which in a sentence.\n",
                "\n",
                "### Exercise 2: Different Datasets\n",
                "Try the sentiment classifier on different datasets (tweets, reviews).\n",
                "\n",
                "### Exercise 3: Compare Models\n",
                "Compare BERT, DistilBERT, and RoBERTa on the same task.\n",
                "\n",
                "### Exercise 4: Custom Transformer\n",
                "Build a complete transformer from scratch for a simple task.\n",
                "\n",
                "---\n",
                "\n",
                "**End of Day 3 - Transformers & Attention Mechanisms** üéâ"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
